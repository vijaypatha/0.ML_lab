{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'filename', 'target']\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_boston()\n",
    "print(dir(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "print(data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regressors \n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y\n",
    "X_LR = data.data\n",
    "y_LR = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_Test Split\n",
    "X_LR_train,X_LR_test,y_LR_train,y_LR_test = train_test_split(X_LR,y_LR,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit\n",
    "reg.fit(X_LR_train,y_LR_train)\n",
    "#Predict \n",
    "y_LR_pred = reg.predict(X_LR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Actual  Predicted\n",
      "0      23.6  28.648960\n",
      "1      32.4  36.495014\n",
      "2      13.6  15.411193\n",
      "3      22.8  25.403213\n",
      "4      16.1  18.855280\n",
      "..      ...        ...\n",
      "147    17.1  17.403672\n",
      "148    14.5  13.385941\n",
      "149    50.0  39.983425\n",
      "150    14.3  16.682863\n",
      "151    12.6  18.285618\n",
      "\n",
      "[152 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'Actual': y_LR_test, 'Predicted': y_LR_pred}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Regression metrics\n",
    "explained_variance=metrics.explained_variance_score(y_LR_test, y_LR_pred) # The best possible score is 1.0, lower values are worse.\n",
    "mean_absolute_error=metrics.mean_absolute_error(y_LR_test, y_LR_pred) #a risk metric corresponding to the expected \n",
    "# value of the absolute error loss\n",
    "mse=metrics.mean_squared_error(y_LR_test, y_LR_pred) # a risk metric corresponding to the expected \n",
    "#value of the squared (quadratic) error or loss\n",
    "median_absolute_error=metrics.median_absolute_error(y_LR_test, y_LR_pred)\n",
    "r2=metrics.r2_score(y_LR_test, y_LR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.7113\n",
      "r2:  0.7112\n",
      "MAE:  3.1627\n",
      "MSE:  21.5174\n",
      "RMSE:  4.6387\n"
     ]
    }
   ],
   "source": [
    "print('explained_variance: ', round(explained_variance,4))    \n",
    "print('r2: ', round(r2,4))\n",
    "print('MAE: ', round(mean_absolute_error,4))\n",
    "print('MSE: ', round(mse,4))\n",
    "print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y\n",
    "X_RR = data.data\n",
    "y_RR = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test\n",
    "X_RR_train, X_RR_test,y_RR_train,y_RR_test = train_test_split(X_RR, y_RR,test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate\n",
    "ridge =Ridge()\n",
    "param_grid={'alpha': [0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 ,\n",
    "       0.65, 0.7 , 0.75]}\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit \n",
    "ridge_cv.fit(X_RR_train,y_RR_train)\n",
    "\n",
    "#predict\n",
    "y_RR_pred = ridge_cv.predict(X_RR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Actual  Predicted\n",
      "0      23.6  28.593002\n",
      "1      32.4  36.452759\n",
      "2      13.6  15.304163\n",
      "3      22.8  25.371201\n",
      "4      16.1  18.911681\n",
      "..      ...        ...\n",
      "147    17.1  17.482869\n",
      "148    14.5  13.393860\n",
      "149    50.0  39.935814\n",
      "150    14.3  16.737009\n",
      "151    12.6  18.376088\n",
      "\n",
      "[152 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'Actual': y_RR_test, 'Predicted': y_RR_pred}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7130103046471367\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(ridge_cv.best_score_)\n",
    "print(ridge_cv.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric\n",
    "from sklearn import metrics\n",
    "# Regression metrics\n",
    "explained_variance=metrics.explained_variance_score(y_RR_test, y_RR_pred) # The best possible score is 1.0, lower values are worse.\n",
    "mean_absolute_error=metrics.mean_absolute_error(y_RR_test, y_RR_pred) #a risk metric corresponding to the expected \n",
    "# value of the absolute error loss\n",
    "mse=metrics.mean_squared_error(y_RR_test, y_RR_pred) # a risk metric corresponding to the expected \n",
    "#value of the squared (quadratic) error or loss\n",
    "median_absolute_error=metrics.median_absolute_error(y_RR_test, y_RR_pred)\n",
    "r2=metrics.r2_score(y_RR_test, y_RR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.7104\n",
      "r2:  0.7103\n",
      "MAE:  3.1624\n",
      "MSE:  21.5851\n",
      "RMSE:  4.646\n"
     ]
    }
   ],
   "source": [
    "print('explained_variance: ', round(explained_variance,4))    \n",
    "print('r2: ', round(r2,4))\n",
    "print('MAE: ', round(mean_absolute_error,4))\n",
    "print('MSE: ', round(mse,4))\n",
    "print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y\n",
    "X_LA = data.data\n",
    "y_LA = data.target\n",
    "\n",
    "#train_test\n",
    "X_LA_train, X_LA_test,y_LA_train,y_LA_test = train_test_split(X_LA, y_LA,test_size=0.3, random_state = 42)\n",
    "\n",
    "#Initiate\n",
    "lasso = Lasso()\n",
    "param_grid={'alpha': [0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 ,\n",
    "       0.65, 0.7 , 0.75]}\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=4)\n",
    "\n",
    "#fit \n",
    "lasso_cv.fit(X_LA_train,y_LA_train)\n",
    "\n",
    "#predict\n",
    "y_LA_pred = lasso_cv.predict(X_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7015959632760466\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(lasso_cv.best_score_)\n",
    "print(lasso_cv.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.6919\n",
      "r2:  0.6918\n",
      "MAE:  3.2674\n",
      "MSE:  22.9638\n",
      "RMSE:  4.7921\n"
     ]
    }
   ],
   "source": [
    "#metric\n",
    "from sklearn import metrics\n",
    "# Regression metrics\n",
    "explained_variance=metrics.explained_variance_score(y_LA_test, y_LA_pred) # The best possible score is 1.0, lower values are worse.\n",
    "mean_absolute_error=metrics.mean_absolute_error(y_LA_test, y_LA_pred) #a risk metric corresponding to the expected \n",
    "# value of the absolute error loss\n",
    "mse=metrics.mean_squared_error(y_LA_test, y_LA_pred) # a risk metric corresponding to the expected \n",
    "#value of the squared (quadratic) error or loss\n",
    "median_absolute_error=metrics.median_absolute_error(y_LA_test, y_LA_pred)\n",
    "r2=metrics.r2_score(y_LA_test, y_LA_pred)\n",
    "\n",
    "print('explained_variance: ', round(explained_variance,4))    \n",
    "print('r2: ', round(r2,4))\n",
    "print('MAE: ', round(mean_absolute_error,4))\n",
    "print('MSE: ', round(mse,4))\n",
    "print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y\n",
    "X_ER = data.data\n",
    "y_ER = data.target\n",
    "\n",
    "#train_test\n",
    "X_ER_train, X_ER_test,y_ER_train,y_ER_test = train_test_split(X_ER, y_ER,test_size=0.3, random_state = 42)\n",
    "\n",
    "#Initiate\n",
    "elastic = ElasticNet()\n",
    "param_grid={'alpha': [0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 ,\n",
    "       0.65, 0.7 , 0.75]}\n",
    "elastic_cv = GridSearchCV(elastic, param_grid, cv=4)\n",
    "\n",
    "#fit \n",
    "elastic_cv.fit(X_ER_train,y_ER_train)\n",
    "\n",
    "#predict\n",
    "y_ER_pred = elastic_cv.predict(X_ER_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7026117471583591\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(elastic_cv.best_score_)\n",
    "print(elastic_cv.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.6924\n",
      "r2:  0.6923\n",
      "MAE:  3.2952\n",
      "MSE:  22.9266\n",
      "RMSE:  4.7882\n"
     ]
    }
   ],
   "source": [
    "#metric\n",
    "from sklearn import metrics\n",
    "# Regression metrics\n",
    "explained_variance=metrics.explained_variance_score(y_ER_test, y_ER_pred) # The best possible score is 1.0, lower values are worse.\n",
    "mean_absolute_error=metrics.mean_absolute_error(y_ER_test, y_ER_pred) #a risk metric corresponding to the expected \n",
    "# value of the absolute error loss\n",
    "mse=metrics.mean_squared_error(y_ER_test, y_ER_pred) # a risk metric corresponding to the expected \n",
    "#value of the squared (quadratic) error or loss\n",
    "median_absolute_error=metrics.median_absolute_error(y_ER_test, y_ER_pred)\n",
    "r2=metrics.r2_score(y_ER_test, y_ER_pred)\n",
    "\n",
    "print('explained_variance: ', round(explained_variance,4))    \n",
    "print('r2: ', round(r2,4))\n",
    "print('MAE: ', round(mean_absolute_error,4))\n",
    "print('MSE: ', round(mse,4))\n",
    "print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.33470103e-01  3.58089136e-02  4.95226452e-02  3.11983512e+00\n",
      " -1.54170609e+01  4.05719923e+00 -1.08208352e-02 -1.38599824e+00\n",
      "  2.42727340e-01 -8.70223437e-03 -9.10685208e-01  1.17941159e-02\n",
      " -5.47113313e-01]\n"
     ]
    }
   ],
   "source": [
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
      "             estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True,\n",
      "                                  l1_ratio=0.5, max_iter=1000, normalize=False,\n",
      "                                  positive=False, precompute=False,\n",
      "                                  random_state=None, selection='cyclic',\n",
      "                                  tol=0.0001, warm_start=False),\n",
      "             iid='warn', n_jobs=None,\n",
      "             param_grid={'alpha': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\n",
      "                                   0.5, 0.55, 0.6, 0.65, 0.7, 0.75]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(elastic_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
